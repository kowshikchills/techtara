<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/techtara.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/techtara.github.io/" rel="alternate" type="text/html" /><updated>2021-03-28T19:49:21+05:30</updated><id>http://localhost:4000/techtara.github.io/feed.xml</id><title type="html">Tech Tara</title><subtitle></subtitle><entry><title type="html">Tech Tara: For Women In Tech</title><link href="http://localhost:4000/techtara.github.io/Tech-Tara-For-Women-In-Tech/" rel="alternate" type="text/html" title="Tech Tara: For Women In Tech" /><published>2021-03-28T00:00:00+05:30</published><updated>2021-03-28T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Tech-Tara-For-Women-In-Tech</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Tech-Tara-For-Women-In-Tech/">&lt;p&gt;Welcome to Tech Tara community! Its your space to explore what you need, share what you have and support many others like you. Being a member of this community is to count you in building our network for encouraging women participation in tech roles. Though we have 30–35% of graduates in science and tech, minor percent of them are getting into actual jobs and research related to their area of study.
We have observed a gap where many aspiring girls and women are pushing their limits to get into technical roles, but due to some reasons they are unable to achieve it. This is a platform to bridge the gap and guide them to be successful. You can become a role model to the ones seeking inspiration or you can find many role models.
We would like your participation in sharing your journeys, roadblocks and most importantly you determination that could inspire&lt;/p&gt;</content><author><name>dharani</name></author><category term="mission" /><summary type="html">Welcome to Tech Tara community! Its your space to explore what you need, share what you have and support many others like you. Being a member of this community is to count you in building our network for encouraging women participation in tech roles. Though we have 30–35% of graduates in science and tech, minor percent of them are getting into actual jobs and research related to their area of study. We have observed a gap where many aspiring girls and women are pushing their limits to get into technical roles, but due to some reasons they are unable to achieve it. This is a platform to bridge the gap and guide them to be successful. You can become a role model to the ones seeking inspiration or you can find many role models. We would like your participation in sharing your journeys, roadblocks and most importantly you determination that could inspire</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/title_pic.jpg" /></entry><entry><title type="html">ALL About ML- Theroy and Code !</title><link href="http://localhost:4000/techtara.github.io/All-About-ML/" rel="alternate" type="text/html" title="ALL About ML- Theroy and Code !" /><published>2021-01-02T00:00:00+05:30</published><updated>2021-01-02T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/All%20About%20ML</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/All-About-ML/">&lt;h2 id=&quot;all-about-ml--theory-and-code&quot;&gt;All About ML — Theory and Code&lt;/h2&gt;

&lt;h3 id=&quot;ml-course-in-blogs--important-machine-learning-concepts-explained-in-detail-for-beginners&quot;&gt;ML Course in Blogs — Important Machine Learning concepts explained in detail for beginners&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2708/1*BgQm4m39PbLfvdnzwWKz3Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/linear-regression-d41a6a5dcab6&quot;&gt;All About ML — Part 1: Detailed explanation of Linear Regression&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/lasso-and-ridge-regularization-a0df473386d5&quot;&gt;All About ML — Part 2: Lasso and Ridge Regularization&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/logistic-regression-8510e9bc477a&quot;&gt;All About ML — Part 3: Logistic Regression&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/evaluation-metrics-in-classification-algorithms-79c036a131cb&quot;&gt;All About ML — Part 4: Evaluation metrics in classification algorithms&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/decision-trees-796395a2d37b&quot;&gt;All About ML — Part 5: Decision Trees&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/bagging-random-forests-and-boosting-8c728e91a85d&quot;&gt;All About ML — Part 6: Bagging, Random Forests and Boosting&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/support-vector-machines-e94cf6854c75&quot;&gt;All About ML — Part 7: Support Vector Machines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/all-about-ml/understanding-principal-component-analysis-pca-556778324b0e&quot;&gt;All About ML — Part 8: Understanding Principal Component Analysis — PCA&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://dharanichowdary25.medium.com/na%C3%AFve-bayes-algorithm-335858137b01&quot;&gt;All About ML — Part 9: Naïve Bayes Algorithm&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/strategies-for-customer-retention-a-cox-survival-model-treatment-6fc008347dc9&quot;&gt;All About ML —Part 10: ML Use Case — Strategies for Customer Retention: A Cox Survival Model Treatment&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>dharani</name></author><category term="featured" /><summary type="html">All About ML — Theory and Code</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/ml.jpg" /></entry><entry><title type="html">NLP Zero to One !</title><link href="http://localhost:4000/techtara.github.io/NLP-Zero-to-One-Full-Course/" rel="alternate" type="text/html" title="NLP Zero to One !" /><published>2021-01-01T00:00:00+05:30</published><updated>2021-01-01T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/NLP%20Zero%20to%20One%20Full%20Course</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/NLP-Zero-to-One-Full-Course/">&lt;h2 id=&quot;nlp-zero-to-one-full-course&quot;&gt;NLP Zero to One: Full Course&lt;/h2&gt;

&lt;h3 id=&quot;simple-clear-and-precise-explanations-of-most-important-nlp-topics-covered-in-15-blogs-each-under-5-minutes-only&quot;&gt;Simple, Clear and Precise Explanations of Most Important NLP Topics Covered in 15 blogs (Each Under 5 Minutes Only!)&lt;/h3&gt;

&lt;h2 id=&quot;series&quot;&gt;Series&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-basics-part-1-30-35c3f6bc7097?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Basics (Part 1/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-sparse-document-representations-part-2-30-d7ce30b96d63?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One : Sparse Document Representations (Part 2/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-theory-basics-part-3-30-baa8cbbe271d?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Deep Learning Theory Basics (Part 3/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-training-procedure-part-4-30-c8d1e3ba0db6?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Deep Learning Training Procedure (Part 4/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-dense-representations-word2vec-part-5-30-9b38c5ccfbfc?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Dense Representations, Word2Vec (Part 5/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Count based Embeddings, GloVe (Part 6/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-training-embeddings-using-gensim-and-visualisation-part-7-30-f0540e976568?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Training Embeddings using Gensim and Visualisation (Part 7/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-recurrent-neural-networks-basics-part-8-30-ca77af9d47ff?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Recurrent Neural Networks Basics Part(8/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-lstm-part-9-40-98e8cc4c296d?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: LSTM Part(9/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-bi-directional-lstm-part-10-30-cab0eab65533?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Bi-Directional LSTM Part(10/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7?source=your_stories_page-------------------------------------&quot;&gt;NLP Theory and Code: Encoder-Decoder Models (Part 11/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-attention-mechanism-part-12-30-c5c36670c81f?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Attention Mechanism (Part 12/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-transformers-part-13-30-5cd5a3ddd93b?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Transformers (Part 13/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-bert-part-14-40-691ef069712f?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: BERT (Part 14/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-knowledge-graphs-part-15-40-df278d91c635?source=your_stories_page-------------------------------------&quot;&gt;NLP Zero to One: Knowledge Graphs Part(15/30)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2834/1*8Beuz1nMOAgZRT_x8Kslhw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks for your time/&lt;/p&gt;</content><author><name>kowshik</name></author><category term="featured" /><summary type="html">NLP Zero to One: Full Course</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/nlp.jpg" /></entry><entry><title type="html">Noisy Networks For Exploration</title><link href="http://localhost:4000/techtara.github.io/Noisy-Networks-For-Exploration/" rel="alternate" type="text/html" title="Noisy Networks For Exploration" /><published>2020-08-06T00:00:00+05:30</published><updated>2020-08-06T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Noisy%20Networks%20For%20Exploration</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Noisy-Networks-For-Exploration/">&lt;h4 id=&quot;a-simple-powerful-and-alternate-exploration-approach&quot;&gt;A Simple, Powerful and Alternate Exploration Approach&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/abs/1706.10295&quot;&gt;Noisy Networks for Exploration&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Authors&lt;/em&gt;&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fortunato%2C+M&quot;&gt;Meire Fortunato&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Azar%2C+M+G&quot;&gt;Mohammad Gheshlaghi Azar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Piot%2C+B&quot;&gt;Bilal Piot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Menick%2C+J&quot;&gt;Jacob Menick&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Osband%2C+I&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Graves%2C+A&quot;&gt;Alex Graves&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mnih%2C+V&quot;&gt;Vlad Mnih&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Munos%2C+R&quot;&gt;Remi Munos&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hassabis%2C+D&quot;&gt;Demis Hassabis&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pietquin%2C+O&quot;&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Blundell%2C+C&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Legg%2C+S&quot;&gt;Shane Legg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Summary by&lt;/em&gt;&lt;/strong&gt;: &lt;a href=&quot;undefined&quot;&gt;Kowshik chilamkurthy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2660/1*pnTVa_iBypl3WsCWx-j8jA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;any-suggestions-and-feedback&quot;&gt;Any suggestions and feedback?&lt;/h4&gt;
&lt;h4 id=&quot;drop-a-mail-kowshikchilamkurthygmailcom&quot;&gt;drop a mail: kowshikchilamkurthy@gmail.com&lt;/h4&gt;
&lt;h4 id=&quot;thanks-&quot;&gt;Thanks !&lt;/h4&gt;</content><author><name>kowshik</name></author><category term="Reinforcement Learning" /><summary type="html">A Simple, Powerful and Alternate Exploration Approach</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/2660/1*pnTVa_iBypl3WsCWx-j8jA.png" /></entry><entry><title type="html">Read this blog before you think you know divisibility completely !</title><link href="http://localhost:4000/techtara.github.io/Read-this-blog-before-you-think-you-know-divisibility-completely/" rel="alternate" type="text/html" title="Read this blog before you think you know divisibility completely !" /><published>2020-08-05T00:00:00+05:30</published><updated>2020-08-05T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Read%20this%20blog%20before%20you%20think%20you%20know%20divisibility%20completely</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Read-this-blog-before-you-think-you-know-divisibility-completely/">&lt;h4 id=&quot;understanding-what-numbers-hide-from-you&quot;&gt;Understanding what numbers hide from you&lt;/h4&gt;

&lt;p&gt;The intention to write this blog is not to dwell too much upon the theories. I don’t plan to confuse reader’s minds providing useless proofs to obvious things. My main motivation is to put these amazing concepts the way I’d like to teach my young-self.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;let me quickly discuss divisibility and GCD(greatest common divisor) and also introduce few notations here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/1*MvIQBKkXxBheHQINUtrDGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;em&gt;Divisibility:&lt;/em&gt;&lt;/strong&gt; An integer b is divisible by a integer a, not zero, if there is an integer x such that b=ax, and we write **a&lt;/td&gt;
          &lt;td&gt;b.**&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;GCD&lt;/em&gt;&lt;/strong&gt;: The greatest common divisor (gcd) of two or more integers, which are not all zero, is the largest positive integer that divides each of the integers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For two integers a, b, the greatest common divisor of a and b is denoted &lt;strong&gt;(a,b)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-gcd-hide-from-you-&quot;&gt;What GCD Hide From You ?&lt;/h2&gt;

&lt;p&gt;I will introduce the concept which I liked the most about GCD. Believe me, you might not believe that these concepts are truly generalised till you see a proper mathematical proof ( &lt;em&gt;which I will provide&lt;/em&gt; 😉).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2322/1*LTiNm-DBkR010PfCqcr9qA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Wait! but it can just be another coincidence right?
Not really what you just saw is truly generalised and magnificent result.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;greatest-common-divisor-of-b-and-c-is-the-least-positive-value-of-bxcy-where-xy-range-all-over-integers&quot;&gt;&lt;strong&gt;&lt;em&gt;Greatest common divisor of b and c is the least positive value of bx+cy where x,y range all over integers.&lt;/em&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;proof&quot;&gt;Proof&lt;/h2&gt;

&lt;p&gt;Lets carryout the proof in two steps. In the first step, we prove that the least positive number &lt;em&gt;l *that obtained from *b𝔁+ c𝒚 ∀ *𝔁,𝒚 ɛ𝐼 is divisible by b,c. In the second step we prove that *l&lt;/em&gt; is GCD of b,c i.e &lt;em&gt;l&lt;/em&gt; = (b,c)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Step 1. &lt;em&gt;l&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;b and &lt;em&gt;l&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;c&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2462/1*BC2HTI1f2Xs9qZfxTPKeng.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Step 2. &lt;em&gt;l&lt;/em&gt; = (b,c)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2314/1*HktMPqybtj2vsSIfyo0M_Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2374/1*YPWbhElnyjMfDPkYGnWNTQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;some-extraordinary-outcomes-of-the-result-&quot;&gt;Some Extraordinary Outcomes of the Result !&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;1-a-b----a-ba𝔁-for-any-integer-𝔁&quot;&gt;1. (a, b ) = ( a, b+a𝔁) for any integer 𝔁&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2374/1*enDBFMqy-MJ5p_sui2NG9A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;2-gcd-n--1--n11--1&quot;&gt;2. GCD (n! + 1 , (n+1)!+1) = 1&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2448/1*U_K-MCphPGxgbtt3JmXpYQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;few-examples-&quot;&gt;Few Examples !&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2450/1*cJnj2AEzDvBHKW_m9hY1iA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Understanding numbers has always been a perpetual pursuit for not just mathematicians but also for many eager minds. This blog is just an selfless and decent attempt to shed some light on the most original concepts in everyday topic like divisibility. The concepts which are introduced to the readers in this blog are generalised and provable, but there many concepts which are waiting to get discovered.&lt;/p&gt;

&lt;p&gt;Suggestions are most welcome. Also do comment for clarifications and questions.&lt;/p&gt;

&lt;p&gt;Thanks for your time :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;An Introduction to Theory of Number&lt;/em&gt; by Iven Niven, Herbert and Hugh&lt;/li&gt;
&lt;/ol&gt;</content><author><name>kowshik</name></author><category term="features" /><summary type="html">Understanding what numbers hide from you</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/4800/1*SkxrzB4wtc-16Yc28W4-Wg.jpeg" /></entry><entry><title type="html">Reinforcement learning for Covid- 19- Simulation and Optimal Policy</title><link href="http://localhost:4000/techtara.github.io/Reinforcement-learning-for-Covid-19-Simulation-and-Optimal-Policy/" rel="alternate" type="text/html" title="Reinforcement learning for Covid- 19- Simulation and Optimal Policy" /><published>2020-06-24T00:00:00+05:30</published><updated>2020-06-24T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Reinforcement%20learning%20for%20Covid-%2019:%20Simulation%20and%20Optimal%20Policy</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Reinforcement-learning-for-Covid-19-Simulation-and-Optimal-Policy/">&lt;p&gt;While the ML community is wondering how they could help the war against the COVID-19 pandemic, I decided to use reinforcement learning to tackle this crisis. This investigation yielded some interesting results in finding the set of optimal actions to fight virus spread.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Imagine you are playing a &lt;strong&gt;pandemic control game.&lt;/strong&gt; Your objective is to control the spreading of the virus with the least economic disruption. You can choose between a multitude of actions like ‘close all infected residential areas’, ‘run tests in infected areas’, ‘lockdown’ etc.&lt;/p&gt;

&lt;p&gt;But the immediate question is: how do I quantify economic disruption? Fairly, we can assume that wider the restriction on the movement of the people, the worse the economic health. So our objective is to control the virus spread with the least impediment on the movement of the population.&lt;/p&gt;

&lt;p&gt;What if an algorithm gives you a trained agent that can take actions on your behalf to achieve the goals you set? Would you not employ such an intelligent agent to curb the virus spread? The subject of reinforcement learning(RL) is around modeling such an intelligent agent.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;the-most-exciting-part-of-this-modelling-is-that-we-can-design-an-agent-that-curbs-the-virus-spread-in-the-long-term-with-the-least-disruption-to-the-economic-activity&quot;&gt;The most exciting part of this modelling is that we can design an agent that curbs the virus spread in the long term with the least disruption to the economic activity.&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-reinforcement-learning&quot;&gt;2. Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Reinforcement Learning is a subfield of machine learning that teaches an agent how to choose an action from its action space. It interacts with an environment, in order to maximize rewards over time. Complex enough? let’s break this definition for better understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The program you train, with the aim of doing a job you specify.&lt;br /&gt; 
&lt;strong&gt;Environment&lt;/strong&gt;: The world in which the agent performs actions.&lt;br /&gt; 
&lt;strong&gt;Action&lt;/strong&gt;: A move made by the agent, which *causes a change *in the environment.&lt;br /&gt; 
&lt;strong&gt;Rewards&lt;/strong&gt;: The evaluation of an action, which is like feedback.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In any RL modelling task, it’s imperative to define these &lt;strong&gt;4 essential elements&lt;/strong&gt;. Before we define these elements for our Covid-19 problem, let’s first try to understand with an example: &lt;em&gt;how agent learn actions in an environment?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: Program controlling the movement of limbs&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Environment&lt;/strong&gt;: The real world simulating gravity and laws of motion&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Action&lt;/strong&gt;: Move limb L with Θ degrees&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Reward&lt;/strong&gt;: Positive when it approaches destination; negative when it falls down.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Agents learn in an interactive environment by &lt;strong&gt;trial and error&lt;/strong&gt; using feedback (Reward) from its own actions and experiences. Agent essentially tries different actions on the environment and learns from the feedback that it gets back. The goal is to find a suitable action policy that would maximize the &lt;strong&gt;total cumulative reward&lt;/strong&gt; of the agent.&lt;/p&gt;

&lt;h2 id=&quot;3-pandemic-control-problem&quot;&gt;3. Pandemic Control Problem&lt;/h2&gt;

&lt;p&gt;Now let’s define these 4 essential elements for our pandemic control problem:
&lt;strong&gt;Agent&lt;/strong&gt;: A Program controlling the movement of the citizens through different actions.&lt;br /&gt; 
&lt;strong&gt;Environment&lt;/strong&gt;: The virtual city where the virus is spreading. By restricting the citizen’s movement, spread dynamics can be altered.&lt;br /&gt; 
&lt;strong&gt;Action&lt;/strong&gt;: Control the movement of the citizens.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Rewards&lt;/strong&gt;: minimise infected from virus spread (pandemic control) +minimise people quarantined( least economic disruption)+ minimise people dead&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now we need to code-up and discuss each element of this optimal control problem. let’s start with pandemic simulation environment.&lt;/p&gt;

&lt;h2 id=&quot;4-pandemic-simulation-environment&quot;&gt;4. Pandemic Simulation Environment&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;model-the-whole-pandemic-transmission-dynamics-as-interactions-between-different-components&quot;&gt;Model the whole pandemic transmission dynamics as interactions between different components.&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;p&gt;Though there are a large number of pandemic simulation models, I decided to use my own simulation model drawing inspiration from the network model. I choose not to use the standard model because of the following reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In existing simulation models, the transmission dynamics of the virus does not react to the actions taken by the decision maker/agent. (eg. How would closing public transport impact virus spreading).&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Existing transmission models doesn’t output a comprehensive observation on the state of the city.&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to prepare such an environment that overcomes above-mentioned shortcomings, I decided to break the whole pandemic transmission dynamics into interactions between different &lt;strong&gt;components&lt;/strong&gt;.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s discuss these components and their respective assumptions of pandemic simulation environment. We will classify these components into Demographic Components, Transmission Dynamics, Contagious Components.&lt;/p&gt;

&lt;h3 id=&quot;demographic-components&quot;&gt;Demographic Components&lt;/h3&gt;

&lt;p&gt;These are basic components of the simulation model on which the whole transmission dynamics are built. We will create a closed city where we intend to simulate the virus spread. There are assumptions considered about this city, such that the simulation process is less computationally expensive and also close to reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transmission-dynamics&quot;&gt;Transmission Dynamics&lt;/h3&gt;

&lt;p&gt;These transmission dynamics decide the extent and intensity of the virus spread. We can simulate any pandemic using these transmission dynamics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-4.gif&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can clearly visualize: Infected citizen makes the daily trip and he/she infects other citizens who came in &lt;strong&gt;contact&lt;/strong&gt; with him with the &lt;strong&gt;probability of transmission&lt;/strong&gt; at each unit. 
We essentially need to define how many citizens come in contact with the infected and what is the probability of transmission at each unit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contagious-components-and-simulation-results&quot;&gt;Contagious Components and Simulation Results&lt;/h3&gt;

&lt;p&gt;These contagious components help us build an environment. For a decision maker to take actions to curb the virus spread, he must understand the state of the infected city&lt;em&gt;( eg. number of citizens infected, number of residential areas infected, number of citizens quarantined ,etc).&lt;/em&gt; 
These components facilitate the logging of infected/interaction information in a structured manner. We use the compartment model for simulation. 
Let’s simulate a simple compartment model with infinite hospital capacity. We will randomly infect 3 citizens and simulate a pandemic following the above transmission dynamics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-6.gif&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contagious Compartment:&lt;/strong&gt; All those active citizens who are infected and contagious are included in this list&lt;br /&gt; 
&lt;strong&gt;Recognized Compartment:&lt;/strong&gt; All those infected who came to the governments notice.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Hospitalized Compartment&lt;/strong&gt;: All those infected citizens recognized by the government will be put in the hospital. Once the infected citizen enter this list, he will be removed from the Contagious Compartment.&lt;br /&gt; 
&lt;strong&gt;Hospital Infrastructure Capacity&lt;/strong&gt;: The capacity of the hospital is limited. Once the capacity reaches, further infected citizens cannot enter the Hospitalized Compartment. This is a very important variable in our simulation, which you will see in plot 6.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Death&lt;/strong&gt;: Infected will be dead as the days progress with the probability proportional to his age&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at the simulation results for the pandemic in a city of 1L population and with infinite hospital Infrastructure Capacity and limited(500) capacity. Also, we need to compare it with standard epidemiological models.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a simple epidemiological model. The “ contagious line” in my simulation model(&lt;strong&gt;Plot 6&lt;/strong&gt;) is closer to the “infected line” in SIR model(Plot 7). This clearly implies that the pandemic simulation is accurate.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;5-actions&quot;&gt;5. Actions&lt;/h2&gt;

&lt;p&gt;The need for creating a new environment for the pandemic problem is essentially because we ideally want our pandemic simulation environment to react to the actions taken by the decision maker. So defining action space is as important as defining the environment. 
So by defining wide action space, we are enriching the decision maker’s choices to curb the virus spread.&lt;/p&gt;

&lt;p&gt;The virus spread can be effectively curbed by:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Restricting the movement of the citizens &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Conducting the tests on probable citizens, so that infected citizens come to the government’s note before the symptoms kick-in.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You will now clearly see why I introduced the concept of transmission dynamics. By restricting the movement of the citizens, they are not susceptible to infection anymore. This condition can be easily embedded into the simulation and the dynamics of the virus spread change accordingly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are the actions defined for the decision maker. 
For example, if the decision maker chooses action: 8 (&lt;strong&gt;lockdown&lt;/strong&gt;): then all the citizens in the city cannot move.&lt;/p&gt;

&lt;p&gt;The idea behind defining this action space is that we want to find the most &lt;strong&gt;optimal action policy&lt;/strong&gt; of restricting citizen’s movement. We can design more actions, but for now, we limit to this action space.&lt;/p&gt;

&lt;h2 id=&quot;6-agent-and-rewards&quot;&gt;6. Agent and Rewards&lt;/h2&gt;

&lt;p&gt;Out of 4 essential elements of Reinforcement Learning, we discussed &lt;em&gt;1. Environment 2. Actions&lt;/em&gt; for our pandemic control problem. Let’s discuss agent and reward in this section.&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;agent&lt;/strong&gt; is essentially a program you train, with the aim of doing a job you specify. &lt;em&gt;But how do we specify the job&lt;/em&gt;? *How can an agent understand your(decision maker) objectives? *The answer is through &lt;strong&gt;reward&lt;/strong&gt;. The agent always try to find out the action policy that maximize the cumulative sum of rewards. So if we can tie the goals of the pandemic control problem with the reward function, we can train an agent which achieves goals for us.&lt;/p&gt;

&lt;p&gt;Let’s reiterate our &lt;strong&gt;objective&lt;/strong&gt;: To control the virus spread with the least impediment on the movement of the population( least economic disruption). 
So we need to minimize:&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Number of people Infected (𝜨𝒊)&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Number of people quarantined(𝜨𝒒)&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Number of people died because of infection(𝜨𝒅)&lt;br /&gt;
We don’t essentially give equal weights to each number. For example, governments don’t let the economy remain healthy at the cost of citizens.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing must be kept in mind when deciding 𝑤𝒊, 𝑤𝒒, 𝑤𝒅. Apart from their ethical importance, these weights are just numbers. We need to choose them judicially such that the agent actually learns to achieve the objectives we set.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In section 2( RL), we learnt how agent trains. Let’s try to understand the training process in the pandemic control problem. I used the DQN model to train the agent. In this DQN model, the agent tries random actions in the beginning (exploratory) to learn optimal action policy. An interesting concept in this model is &lt;strong&gt;discounted sum of rewards&lt;/strong&gt;: agent gives lesser importance to the immediate rewards and strives to achieve long terms goals.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I will briefly explain this RL model: Q-learning learns the action-value function &lt;em&gt;Q(s, a)&lt;/em&gt;: &lt;strong&gt;how good to take an action at a particular observation&lt;/strong&gt;.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s try to understand Q value: Consider the pandemic simulation environment, for a given observation:&lt;br /&gt;
&lt;em&gt;{infected, hospitalized, dead, exposed, infected houses, average age of infected}&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Agent will learns Q value &lt;strong&gt;(expected rewards)&lt;/strong&gt; for each action ( Total 16 actions). The agent chooses the action with the highest Q value. We will limit the discussion on RL modelling techniques and jump into the results and Interpretation.&lt;/p&gt;

&lt;h2 id=&quot;7-results-and-interpretation&quot;&gt;7. Results and Interpretation&lt;/h2&gt;

&lt;p&gt;Now we reach the end and also the most interesting part of this blog.&lt;/p&gt;

&lt;p&gt;So let’s create a pandemic simulation in a city of size 1 Lakh. We will let the DQN agent take actions from its action space A &lt;em&gt;(plot 8)&lt;/em&gt; to maximize the reward R&lt;em&gt;( Equation 1).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B1-14.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;8-summary&quot;&gt;8. Summary&lt;/h2&gt;

&lt;p&gt;This modelling and simulation can be extended to cities of different sizes. The actions taken by the agent are more intuitive as the agent understands/learns the pandemic simulation environment better. For example, agents choose to do a lot of tests in infected areas at the beginning of the spread. More action spaces and better reward function makes this whole RL modelling even closer to reality.&lt;/p&gt;

&lt;p&gt;As I mentioned in the beginning, the intention behind writing this blog is to explore the possibility of collaboration and help the war against the corona virus spread. If anyone believes that they can contribute to this RL project, please feel free to mail me kowshikchilamkurthy@gmail.com. Also, I would love to take suggestions from you for better simulation and better RL modelling.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;references:&lt;/em&gt;
1.&lt;a href=&quot;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model&quot;&gt;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.mathworks.com/headlines/2019/05/16/robot-quickly-teaches-itself-to-walk-using-reinforcement-learning/&quot;&gt;https://blogs.mathworks.com/headlines/2019/05/16/robot-quickly-teaches-itself-to-walk-using-reinforcement-learning/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;H. S. Rodrigues, M. T. T. Monteiro, and D. F. M. Torres, “Dynamics of dengue epidemics when using optimal control,” &lt;em&gt;Mathematical and Computer Modelling&lt;/em&gt;, vol. 52, no. 9–10, pp. 1667–1673, 2010.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>kowshik</name></author><category term="ReinforcementLearning" /><category term="tutorial" /><summary type="html">While the ML community is wondering how they could help the war against the COVID-19 pandemic, I decided to use reinforcement learning to tackle this crisis. This investigation yielded some interesting results in finding the set of optimal actions to fight virus spread.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/B1-1.jpg" /></entry><entry><title type="html">Improve Survival Time in PUBG- A Cox Statistical Approach</title><link href="http://localhost:4000/techtara.github.io/Improve-Survival-Time-in-PUBG-A-Cox-Statistical-Approach/" rel="alternate" type="text/html" title="Improve Survival Time in PUBG- A Cox Statistical Approach" /><published>2020-06-15T00:00:00+05:30</published><updated>2020-06-15T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Improve%20Survival%20Time%20in%20PUBG:%20A%20Cox%20Statistical%20Approach</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Improve-Survival-Time-in-PUBG-A-Cox-Statistical-Approach/">&lt;h3 id=&quot;a-real-world-application-of-cox-proportional-hazards-model&quot;&gt;A Real World Application of Cox Proportional-Hazards Model&lt;/h3&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;PUBG needs no introduction. It is one of the popular and the most played games right now. Players fight to death until one remains, so it is a survival game. There are pure statistical models to analyse the survival times. Using **PUBG data, **we will try to use one such survival models to understand how different strategies can improve the player’s survival rates.&lt;/p&gt;

&lt;p&gt;This blog is written for tech, non-tech readers and most importantly PUBG players. I will also include my python implementation for the benefit of tech readers. This can be seen as a sequel to my blog: [&lt;em&gt;The Cox Proportional-Hazards Model](https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50) and must read for those who are interested in understanding the mathematical background and python implementation of magical *Cox Proportional-Hazards Model.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will use the data published in Kaggle &lt;a href=&quot;https://www.kaggle.com/skihikingkevin/pubg-match-deaths?select=aggregate&quot;&gt;datasets &lt;/a&gt;where there are over 720,000 PUBG matches. The data log was extracted from &lt;a href=&quot;http://pubg.op.gg/&quot;&gt;pubg.op.gg&lt;/a&gt;, a game tracker website. We will use this data log to understand different modes of game strategies using statistical models and try to figure out the method to evaluate the strategies.&lt;/p&gt;

&lt;h3 id=&quot;a-quick-recap-of-cox-proportional-hazards-model&quot;&gt;A Quick Recap of Cox Proportional-Hazards Model&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Cox proportional-hazards model&lt;/em&gt; is developed by Cox and published in his work[1] in 1972. It is the most commonly used regression model for survival data. The most interesting aspect of this survival modeling is it’s ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates. &lt;em&gt;Note: It must not be confused with linear regression, the assumptions might be linear in both regression and survival analysis but the underlying concepts are different. Methods we employ for parameter estimations of regression model and survival model are very different from each other.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-3.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;H&lt;/em&gt;azard function &lt;strong&gt;λ(t)&lt;/strong&gt;: gives the instantaneous risk of demise at time t&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z: Vector of features/covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt; is called the baseline hazard function&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pubg-problem-setup--data-engineering&quot;&gt;PUBG Problem Setup &amp;amp; Data Engineering&lt;/h2&gt;

&lt;p&gt;Let’s have a look at the raw data before we define the problem setup.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv(‘agg_match_stats_0.csv’)
df.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-4.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feature Description&lt;/em&gt;&lt;strong&gt;: player_size&lt;/strong&gt;: Team Size, &lt;strong&gt;player_dist_ride&lt;/strong&gt;: Distance covered using vehicle by the player , &lt;strong&gt;player_dist_walk&lt;/strong&gt;: Distance walked by the player, &lt;strong&gt;player_kills&lt;/strong&gt;: Number of kills by the player, &lt;strong&gt;players_survive_time&lt;/strong&gt;: time survived by the player&lt;/p&gt;

&lt;h3 id=&quot;problem-setup&quot;&gt;Problem Setup&lt;/h3&gt;

&lt;p&gt;Players in PUBG can choose different strategies to maximise the survival time. We define strategy as a combination of one or more &lt;em&gt;player’s decisions&lt;/em&gt;. Strategies can be something like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Travel extensively with least confrontation with enemies,&lt;/li&gt;
  &lt;li&gt;Use a motorised vehicle most of the time,&lt;/li&gt;
  &lt;li&gt;Only Walk, but confront with enemies more often, or&lt;/li&gt;
  &lt;li&gt;Even something funnier like: Play only afternoons over the weekends😝.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There can be 1000’s of such strategies, some of them might look trivial other might not. Our goal is to find a way to evaluate these strategies based on their survival rates. Apart from raw data provided, we also need to engineer these columns to derive meaningful features(player decisions).&lt;/p&gt;

&lt;h3 id=&quot;data-engineering&quot;&gt;Data Engineering&lt;/h3&gt;

&lt;p&gt;In this section, we will briefly discuss the features needed to be extract from the raw data available to use. These features can be simply seen as the decisions taken by the player. Let’s list them and also look at the distributions for some of these features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-5.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
df = pd.read_csv('agg_match_stats_0.csv')
df_features = create_features(df) #func is defined at end of blog
df_features.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-6.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we extracted the features, lets jump into the implementation of cox proportional-hazards model.&lt;/p&gt;

&lt;h2 id=&quot;survival-analysis&quot;&gt;Survival Analysis&lt;/h2&gt;

&lt;p&gt;This is the most interesting section: the implementation of cox model in python with the help of lifelines package. It is very important to know about the impact of features on the survival rates. This would help us in predicting the survival rates of a PUBG player, if we know the associated feature values. The Cox model assumes that each features have an impact on the survival rates.&lt;/p&gt;

&lt;p&gt;One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls for high correlation between covariates.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df, duration_col='player_survive_time', event_col='dead')
cph.plot()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-7.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Coefficients of the features which indicate the measure of the impact on the survival rates of the PUBG player.&lt;/p&gt;

&lt;h3 id=&quot;interpreting-the-summary&quot;&gt;Interpreting the summary&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hazard ratio (HR) given by exp(coef), where coef is the weight corresposing to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &amp;gt; 1, decreases the hazard: &lt;strong&gt;improves the survival&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;weekend_indi&lt;/em&gt;( that is whether player player over weekend or weekday ) doesn’t play any significant role in predicting his survival risk, whereas &lt;em&gt;player_kills&lt;/em&gt; ( number of kills by player) variable plays significant role in predicting survival risk .&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;game size *feature with exp(coef) = 1.0 has &lt;strong&gt;no effect&lt;/strong&gt; on the survival rates: so it implies that the survival of the player does not depend on the *game size.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;*%player_dist_ride *feature with exp(coef) = 1.73 (&amp;gt;1) this is good for survival. So preferring the &lt;strong&gt;vehicle&lt;/strong&gt; instead of &lt;strong&gt;walking&lt;/strong&gt; increases the survival rates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For better understanding of the math behind above deductions, please refer to my earlier blog: &lt;a href=&quot;https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50&quot;&gt;*The Cox Proportional-Hazards Model&lt;/a&gt;. 
*In the next section, we will also see how different features play together to decide the survival rates of the PUBG player.&lt;/p&gt;

&lt;h2 id=&quot;results--visualisation&quot;&gt;Results &amp;amp; Visualisation&lt;/h2&gt;

&lt;p&gt;The best way we understand impact of each features/decision is that we plot the survival curves for single feature/decision i.e., we keep all other player’s decisions unchanged. we use[&lt;strong&gt;plot_covariate_groups()](https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.CoxPHFitter.plot_covariate_groups)&lt;/strong&gt; method and give it the feature of interest, and the values to display. Also we will look at the survival rates for different strategies ( combination of decisions)&lt;/p&gt;

&lt;p&gt;In this section we will discuss&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Survival profiles of Decisions&lt;/li&gt;
  &lt;li&gt;Survival profiles of Strategies&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;survival-profiles-of-decisions&quot;&gt;Survival profiles of Decisions&lt;/h3&gt;

&lt;p&gt;One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival probability than that of its left. Let’s try to understand this with an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-8.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-9.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-10.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpreting plot 3&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It clearly implies that the survival time of PUBG player increases if he choose to walk instead of taking a vehicle&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More the distance he traverses, better his survival rates (which is intuitive)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;survival-profiles-of-strategies&quot;&gt;Survival profiles of Strategies&lt;/h3&gt;

&lt;p&gt;Let’s quickly see the survival profile for different strategies. For example, consider these four strategies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Use vehicles extensively, travel longer distances and kill often&lt;/li&gt;
  &lt;li&gt;Only walk, travel smaller distances and don’t confront with enemies often&lt;/li&gt;
  &lt;li&gt;Do team work, use vehicle less often and travel large distances&lt;/li&gt;
  &lt;li&gt;Select a match with small number of players and kill extensively&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-11.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The values for decisions are fixed as per the above 4 strategies&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B7-12.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even in the real world survival situations, moving and confronting with the enemies is better than staying idle. We can handcraft 1000’s of such strategies and compare their survival behaviours. We can even understand and approximate the human behaviour during survival situations by applying these kind of statistical model on the data extracted from the survival games.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We looked at a real world application of Cox proportional-hazards model. We understood how different strategies impact the survival times of the PUBG player. Out of those strategies we analysed, we found strategy of “&lt;em&gt;using vehicles extensively, travelling longer distances and killing often&lt;/em&gt;” statistically promising the longest survival of a PUBG player in a match. There are also neural network variants of Cox proportional-hazards model, we will look at such neural variant of Cox PH model in my next blog in this series.&lt;/p&gt;

&lt;p&gt;Thanks for your time :)&lt;/p&gt;

&lt;p&gt;Here is the full code for reference:&lt;/p&gt;

&lt;p&gt;#import all the dependencies&lt;/p&gt;

&lt;p&gt;import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats&lt;/p&gt;

&lt;p&gt;#import data set
df = pd.read_csv(‘agg_match_stats_0.csv’)&lt;/p&gt;

&lt;p&gt;#Create new features&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def create_features(df)
    df['player_survive_time'] = df['player_survive_time']/60
    df['date'] = [i.split('+')[0] for i in df['date'].values]
    df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%dT%H:%M:%S')
    df['dayofweek_num']=df['date'].dt.dayofweek  
    df['dayofweek_name']=df['date'].dt.weekday_name
    df['Hour'] = df['date'].dt.hour 
    df['weekend_indi'] = 0       
    df.loc[df['dayofweek_num'].isin([5, 6]), 'weekend_indi'] = 1
    df['time_of_day'] = 0       
    df.loc[df['Hour'].isin([24,1,2,3,4,5,6]), 'time_of_day'] = &quot;LateNight&quot;
    df.loc[df['Hour'].isin([7,8,9,10,11]), 'time_of_day'] = &quot;Morn&quot;
    df.loc[df['Hour'].isin([12,13,14,15,16,17,18,19]), 'time_of_day'] = &quot;Evening&quot;
    df.loc[df['Hour'].isin([20,21,22,23]), 'time_of_day'] = &quot;Night&quot;
    df['%player_dist_ride'] = df['player_dist_ride']/(df['player_dist_ride']+df['player_dist_walk'])
    df['%player_dist_walk'] = df['player_dist_walk']/(df['player_dist_ride']+df['player_dist_walk'])
    df['total distance'] = df['player_dist_ride']+df['player_dist_walk']
    df['only_walk'] = 0       
    df.loc[df['%player_dist_walk'].isin([1]), 'only_walk'] = 1
    for i in ['date','team_id','team_placement','player_name','player_dmg','player_dbno','player_dist_ride',
             'player_dist_walk','Hour','dayofweek_num','dayofweek_name','match_id','match_mode']:
        del df[i]
    one_hot = pd.get_dummies(df['time_of_day'])
    df = df.drop('time_of_day',axis = 1)
    df = df.join(one_hot)
    del df[0]
    df['dead'] = 1
    df = df.dropna()
    return(df)


df_sampled = create_features(df)

from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df_sampled, duration_col='player_survive_time', event_col='dead')


'''

Individual features 
'''
cph.plot_covariate_groups('only_walk', [0,1], cmap='coolwarm')
plt.xlabel('time (minutes)')
plt.ylabel('Survival Curve')

'''
strategies

'''
df_strategy = pd.DataFrame()
df_strategy['game_size'] = [60,60,60,30]
df_strategy['party_size'] = [2,2,2,1]
df_strategy['player_assists'] = [1,0,4,1]
df_strategy['player_kills'] = [6,1,2,5]
df_strategy['weekend_indi'] = [0,0,0,0,]
df_strategy['%player_dist_ride'] = [0.8,0,0.2,0.5]
df_strategy['%player_dist_walk'] = [0.2,1,0.8,0.5]
df_strategy['total distance'] = [9000,3000,7000,4000]
df_strategy['only_walk'] = [0,1,0,0]
df_strategy['Evening'] = [1,0,1,1]
df_strategy['LateNight'] = [0,1,0,1]
df_strategy['Morn'] = [0,0,1,0]
df_strategy['Night'] = [0,0,0,0]
df_strategy.index = ['Strategy 1','Strategy 2','Strategy 3','Strategy 4']

cph.predict_survival_function(df_strategy).plot()
plt.xlabel('time (minutes)')
plt.ylabel('Survival Curve')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>kowshik</name></author><category term="CoxProcess" /><summary type="html">A Real World Application of Cox Proportional-Hazards Model</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/B7-2.jpg" /></entry><entry><title type="html">Fundamentals of Reinforcement Learning</title><link href="http://localhost:4000/techtara.github.io/Fundamentals-of-Reinforcement-Learning/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning" /><published>2020-06-14T00:00:00+05:30</published><updated>2020-06-14T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Fundamentals%20of%20Reinforcement%20Learning</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Fundamentals-of-Reinforcement-Learning/">&lt;h3 id=&quot;learning-decisions-that-makes-the-difference&quot;&gt;&lt;em&gt;Learning decisions that makes the difference&lt;/em&gt;&lt;/h3&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Designing machine that learn to do a job by itself is one of the most researched topic than any other in recent times due to various reasons like increased computational power, availability of resources to experiment etc., This lead to uncover significant innovations that made life simpler. If you just have data then an algorithm will provide insights or you train a model and it recognizes your face and many other use cases that we see around us which are built using Machine Learning and Deep Learning. Reinforcement Learning is burgeoning by gaining a lot of attention due to its proven capability in making &lt;strong&gt;sequential&lt;/strong&gt; &lt;strong&gt;decision&lt;/strong&gt; process.&lt;/p&gt;

&lt;h2 id=&quot;concepts-of-rl&quot;&gt;Concepts of RL&lt;/h2&gt;

&lt;p&gt;Reinforcement Learning basically consists of an &lt;strong&gt;agent&lt;/strong&gt;(decision maker) that tries to learn from a &lt;strong&gt;state&lt;/strong&gt; in a given surrounding that it interacts called &lt;strong&gt;environment&lt;/strong&gt; and changes its state because of some &lt;strong&gt;action&lt;/strong&gt; taken as per the feedback provided by the environment during the &lt;strong&gt;episode&lt;/strong&gt;. This feedback is numerical(positive, negative or zero) and is called a &lt;strong&gt;reward&lt;/strong&gt;. The optimal behavior of an agent is to learn such that it always gets good feedback i.e., maximize the reward by taking suitable actions. So in RL we are providing the scenario to the agent and it can figure out itself or discover how to take decisions in the most applaudable way.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets understand the terms used in RL using the very well known PUBG game as a simple example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The player in PUBG is an &lt;strong&gt;agent&lt;/strong&gt; here and battleground is his &lt;strong&gt;environment&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A complete game played is an &lt;strong&gt;episode&lt;/strong&gt; and walking, running etc are &lt;strong&gt;states&lt;/strong&gt; — helps to pick actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The agent has number of &lt;strong&gt;actions&lt;/strong&gt; to take like moving left, right, front and back, run, fire, kill, bend, jump, change gun etc.,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;reward&lt;/strong&gt; the agent gets here is positive if he kills and zero if he survives with the help of his teammates till the end or negative if he gets shot by other player.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to win the game we have to maximize the reward by taking suitable actions at each time frame. Simply we start from a state, take an action and change to another state and get a reward for that action and repeat the process to learn more about the environment setting.&lt;/p&gt;

&lt;p&gt;There are few challenges RL has before us. Some of them are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Trade-offs: As we understood that agent has to optimize the rewards and also it has to interact continuously with the environment i.e., it needs to explore a lot. This leads to a trade off between exploration and exploitation. It has to choose whether it should keep exploring new states which might result in lower reward or take the path that has already seen and got quite a good set of rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization: Can the agent understand or learn if the actions are good/bad in its previously unseen states.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Delayed consequences: We also need to understand if an agent gives a high reward in a current state, it is because of just this state or a series of decisions that it has taken to reach this state?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are few key concepts that are applicable in RL. A good knowledge on these will let us understand the formulation of agent’s decision process and model of the environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Markov Property:&lt;/strong&gt; If an agent changes from one state to other it is called a transition and the probability at which it makes the transition is called transition probability. Generally if we have all the probabilities of an agent going from one state to the other, then it is represented in a transition matrix. Markov property says “&lt;em&gt;Future is independent of past given present&lt;/em&gt;”. The equation below depicts the probability of transitioning to next state St+1 in time t is only dependent on the current state St and the action taken At and is independent of the history&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the transition probability matrix which has all the transition probabilities of all states. For example, P12 depicts the probability of transitioning from state 1 to state 2 and so on.&lt;/p&gt;

&lt;p&gt;When we traverse through a set of states in the environment which follows Markov Property, then it is called a Markov chain. They might include random set of states in the Markov chain that also have transition probabilities and we can compute the optimal chain that results in high reward.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In RL, we are more concerned about optimizing the total reward that an agent receives from the environment rather than the immediate reward it gets by transitioning from one state to the other. So we measure the optimality using a function called &lt;strong&gt;return(Gt)&lt;/strong&gt; which is sum of rewards the agent received from time t (Eq 1).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In many games like Atari, alpha go, chess or PUBG we know the game is going to terminate after certain time steps. If this is the setting then it is called an &lt;strong&gt;episodic&lt;/strong&gt; task. If we start another game then we are initiating a new episode so episodes are &lt;strong&gt;independent&lt;/strong&gt; of each other. There can also be problems where it is not going to have an end like certain robots that are used for personal assistance which do not terminate until an external signal from environment puts it in termination state. These are called &lt;strong&gt;continuous&lt;/strong&gt; tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In episodic tasks we can calculate the returns which is a total sum of its rewards till the termination but in continuous tasks as there is no termination, the rewards add up to &lt;strong&gt;infinity&lt;/strong&gt; while calculating the returns. So we introduce a discount factor gamma&lt;strong&gt;(ɤ)&lt;/strong&gt; to calculate the returns in continuous tasks by discounting it. It has it values from 0–1. It plays a crucial role in determining if we give importance to immediate rewards or future rewards. If &lt;strong&gt;ɤ=0&lt;/strong&gt; then the attention lies on immediate rewards and if &lt;strong&gt;ɤ=1&lt;/strong&gt; then on future rewards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we have a problem statement which says you get a reward of 1 for performing certain action for next k time steps with a discount factor 0.8 and 0.2 then the returns would be&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We see that Gt with &lt;strong&gt;ɤ=0.8&lt;/strong&gt; is yielding a good return even in future but for &lt;strong&gt;ɤ=0.2 **the returns are high only in the immediate time step and in future it is almost tending to zero. So based on the problem statement we can set **ɤ **that facilitates to decide the importance of either **immediate&lt;/strong&gt; or **future **rewards.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We now know an agent changes its states and gets a reward for that transition. Lets check an example to understand in detail:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a situation where a student is an agent and he have four states Home, School, Class, Movie and a discount factor of 0.8. The probabilities of transitioning from one state to other is shown in blue boxes and rewards are shown in brown boxes. Agent might have many episodes i.e., a sequence of traversing through states. For example,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Home -&amp;gt; School -&amp;gt; Class -&amp;gt; Home -&amp;gt; Terminate — Lets calculate the returns of state Home. G = 3 + 5&lt;em&gt;0.8 + 5&lt;/em&gt;0.8*0.8 = 10.2&lt;/li&gt;
  &lt;li&gt;Home -&amp;gt; School -&amp;gt; School -&amp;gt; Movie -&amp;gt; Home -&amp;gt; Terminate — Returns in this episode is G = 3 + 2&lt;em&gt;0.8 + (-10)&lt;/em&gt;0.8&lt;em&gt;0.8 + 3&lt;/em&gt;0.8&lt;em&gt;0.8&lt;/em&gt;0.8 = — 0.264&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is clear that episode 1 results in high return than episode 2. So it would be feasible to follow it. Returns is a significant concept as it can decide the agents optimal path.&lt;/p&gt;

&lt;h2 id=&quot;markov-reward-processmrp&quot;&gt;Markov Reward Process(MRP):&lt;/h2&gt;

&lt;p&gt;MRP is a Markov process setting which specifies a reward function and a discount factor &lt;strong&gt;ɤ&lt;/strong&gt;. It is formally represented using the tuple (S, P, R, γ) which are listed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;S : A finite state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P : A transition probability model that specifies P(s`&lt;/td&gt;
          &lt;td&gt;s).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;R : A reward function that maps states to rewards (real numbers) R(s) = E[ri&lt;/td&gt;
          &lt;td&gt;si = s] , ∀ i = 0, 1, . . . .(E here is expected value and i is every time step)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;γ: Discount factor — lies between 0 and 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;State Value Function&lt;/strong&gt;: State Value function Vt(s) is the expected sum of returns starting from state s at a time t.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simply put, value function denotes how good it is for an agent to be in that particular state. Transitioning between states that result in a high reward during episodes is an optimal MRP. We have different methods to evaluate V(s). They are&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Monte-Carlo Simulation Method: In MRP, for each episode returns are calculated and are averaged. So State Value function is calculated as Vt(s)=Sum(Gt)/Number of episodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Analytic Solution: If the number of time steps are infinite, then we cannot calculate sum or average of returns. In this process, we define γ&amp;lt;1 and State value function is equal to sum of Immediate Reward(reward obtained for transitioning from State s to s’) and discounted sum of future rewards. The equation can be represented in Matrix form as V=R + γPV. Rearranging gives V by multiplying inverse matrix of (I − γP) with R.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Iterative Solution: In this method we calculate Value Function at time step t by iterating through its previous value functions at time t-1,t-2 etc., We will look into this in depth soon.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;markov-decision-process-mdp&quot;&gt;Markov Decision Process (MDP):&lt;/h2&gt;

&lt;p&gt;An MDP is simply an MRP but with the specifications of a set of actions that an agent can take from each state. It is represented a tuple (S, A, P, R, γ) which denotes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;S : A finite state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A : A finite set of actions which are available from each state s.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P : A transition probability model that specifies P(s`&lt;/td&gt;
          &lt;td&gt;s).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;R : A reward function that maps states to rewards (real numbers) R(s) = E[ri&lt;/td&gt;
          &lt;td&gt;Si = s, Ai=a] , ∀ i = 0, 1, . . . .(state s, action taken a, E here is expected value and i is every time step)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;γ : Discount factor — lies between 0 and 1. An episode of a MDP is thus represented as (s0, a0, r0, s1, a1, r1, s2, a2, r2, . . .).&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In MRP, we have transition probability of going from one state to the other. In MDP, the notation is slightly changed. We define transition probability with respect to action as well P(Si+1&lt;/td&gt;
      &lt;td&gt;Si , ai). An example of robot transitioning between different states also depends on the action it takes if its moving forward, left, right or halted. All the other notations of returns(Gt), discount factor(γ) are exactly the same as referred in MRP.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;policy-and-q-function&quot;&gt;Policy and Q-Function:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Suppose there is a robot which is currently at a state S1, it can take actions left or right with probabilities al and ar respectively for taking left or right which lands in two different states S2 and S3. Value function and rewards of that state are also mentioned and discount factor = 0.8&lt;/p&gt;

&lt;p&gt;Let us calculate the value function of S1:
V1= al(R+γ&lt;em&gt;V2) + ar(R+γ&lt;/em&gt;V3) 
 = al(2+ 0.8&lt;em&gt;10) + ar(1+ 0.8&lt;/em&gt;15) 
 = al&lt;em&gt;10 + ar&lt;/em&gt;13.5&lt;/p&gt;

&lt;p&gt;If a1 = 0.2 and ar = 0.8, then &lt;strong&gt;V1 = 12.8&lt;/strong&gt; and if a1 = 0.8 and ar = 0.2 then &lt;strong&gt;V1 = 10.7&lt;/strong&gt; Clearly giving more probability to take the action &lt;strong&gt;ar&lt;/strong&gt; will give a better result in terms of value of the state S1.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To evaluate how good it is to transition to a state is we use &lt;strong&gt;value function&lt;/strong&gt; but to determine how good is it to take an &lt;strong&gt;action&lt;/strong&gt; ‘a’ from this state? This is where the concept of &lt;strong&gt;Policy&lt;/strong&gt; sets in.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;policy-π&quot;&gt;Policy π:&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Policy is a &lt;strong&gt;decision making&lt;/strong&gt; mechanism in MDP that maps states to actions. For a policy π, if at time t the agent is in state s, it will choose an action a with probability given by π(a&lt;/td&gt;
      &lt;td&gt;s).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given a policy &lt;strong&gt;π&lt;/strong&gt; how can we evaluate if its good or not? The intuition is same as in MRP, we calculate the expected rewards. We can define as below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;State Value Function&lt;/strong&gt; (how good is it to transition to a state): Value function at a given state s of an agent, is the expected returns obtained by following a policy &lt;strong&gt;π&lt;/strong&gt; and reaching to a next state, until we reach a terminal state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;state-action-value-function-or-q--function&quot;&gt;State-Action Value Function or Q — Function:&lt;/h3&gt;

&lt;p&gt;The state-action value function for a state s and action a is defined as the expected return starting from the state St = s at time t and taking the action At = a, and then subsequently following the policy π. It is written mathematically as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/RL1-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So this tells us the value of performing an action a in state s following policy π.&lt;/p&gt;

&lt;p&gt;These are just building blocks of MDP in RL. There are lot more concepts like Bell Man Backup Operator, Finding Optimal Value functions and Policies and Dynamic Programming etc., Lets check them out in my next blog.&lt;/p&gt;

&lt;p&gt;Hope this article has pushed your understanding of RL to some level up.&lt;/p&gt;

&lt;p&gt;Thanks for your time !&lt;/p&gt;</content><author><name>dharani</name></author><category term="RL" /><summary type="html">Learning decisions that makes the difference</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/RL1-2.jpg" /></entry><entry><title type="html">Strategies for Customer Retention: A Cox Survival Model Treatment</title><link href="http://localhost:4000/techtara.github.io/Strategies-for-Customer-Retention-A-Cox-Survival-Model-Treatment/" rel="alternate" type="text/html" title="Strategies for Customer Retention: A Cox Survival Model Treatment" /><published>2020-06-12T00:00:00+05:30</published><updated>2020-06-12T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/%20Strategies%20for%20Customer%20Retention:%20A%20Cox%20Survival%20Model%20Treatment</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Strategies-for-Customer-Retention-A-Cox-Survival-Model-Treatment/">&lt;h4 id=&quot;techniques-to-devise-personalized-strategies-using-statistical-models&quot;&gt;Techniques to devise personalized strategies using statistical models&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Customer churn occurs when customers or subscribers discontinue their association with a company or service. There are many Machine Learning models to predict if a customer is going to churn or not. The problem doesn’t stop there, business has to deploy certain strategies to retain the customers who are at the verge of churn because it’s &lt;a href=&quot;https://www.forbes.com/sites/jiawertz/2018/09/12/dont-spend-5-times-more-attracting-new-customers-nurture-the-existing-ones/#4b86da1f5a8e&quot;&gt;five times cheaper&lt;/a&gt; to retain an existing customer than to acquire a new one. Statistical models can be used to derive and evaluate personalized strategies which is a core challenge in CPG companies.&lt;/p&gt;

&lt;p&gt;We call the event of customer churn as &lt;strong&gt;&lt;em&gt;failure&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;survival time&lt;/em&gt;&lt;/strong&gt; is the time taken for such failure/churn. Survival models are statistical techniques used to estimate the time span taken for an event to occur. Cox Proportional-Hazards model is a popular statistical model for survival analysis. Using churn data set from Kaggle, we will try to use this model to understand customer behavior and compare different strategies that can improve customer retention.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50&quot;&gt;this blog&lt;/a&gt; to understand Mathematical Equations and reason behind using this.&lt;/p&gt;

&lt;h2 id=&quot;road-map-to-enhance-customer-retention-rate&quot;&gt;Road map to enhance customer retention rate:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Customer’s characteristics and demographics play a pivotal role in understanding retention behavior. Our goal is to understand the relation between these features and survival time(time taken to churn). We can plot survival/retention curves that are specific to a customer to gain valuable insights.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Devise personalized strategies(for example, increase incentives/offers)for high-valued customers for different survival risk segments during the time. Our goal is to evaluate and compare how they improve the survival/retention behavior in a customer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;a-quick-recap-of-cox-proportional-hazards-model&quot;&gt;A Quick Recap of Cox Proportional-Hazards Model&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Cox proportional-hazards model&lt;/em&gt; is developed by Cox and published in his work[1] in 1972.The most interesting aspect of this survival modeling is its ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;H&lt;/em&gt;azard function &lt;strong&gt;λ(t)&lt;/strong&gt;: gives the instantaneous risk of demise at time t&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z: Vector of features/covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt; is called the baseline hazard function: Describes how the risk of event changes over time. It is underlying hazard with all covariates equal to 0.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;model-implementation-on-churn-data-set&quot;&gt;Model Implementation on churn data set:&lt;/h2&gt;

&lt;h3 id=&quot;problem-setup--data-engineering&quot;&gt;Problem Setup &amp;amp; Data Engineering&lt;/h3&gt;

&lt;p&gt;I have taken telecom customer churn data set. Lets check the data structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv(‘Data_Churn_Telecom_Cox.csv’)
df.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;These &lt;strong&gt;features&lt;/strong&gt; gives the customer’s &lt;strong&gt;demographics&lt;/strong&gt; and &lt;strong&gt;characteristics&lt;/strong&gt; / &lt;strong&gt;behaviour&lt;/strong&gt;. There are 96 such features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“&lt;strong&gt;Total number of months in service&lt;/strong&gt;” column gives us the survival/retention time of a customer. “&lt;strong&gt;churn&lt;/strong&gt;” column gives whether customer churns or not i.e., event occurrence.&lt;/p&gt;

&lt;h3 id=&quot;data-engineering&quot;&gt;Data Engineering&lt;/h3&gt;

&lt;p&gt;Listed down are the &lt;em&gt;feature engineering steps&lt;/em&gt; and we also look at the distributions for some of these features. Code for each feature engineering step is published at end of the blog.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A fairly simple assumption is &lt;strong&gt;proportional hazards&lt;/strong&gt;, which is crucial in Cox regression that is included in its name(the Cox proportional hazards model). It means that the &lt;em&gt;ratio&lt;/em&gt; of the hazards for any two individuals is constant over time. We drop those features if they don’t pass this condition.&lt;/p&gt;

&lt;h2 id=&quot;survival-analysis&quot;&gt;Survival Analysis&lt;/h2&gt;

&lt;p&gt;Here comes the most interesting section: the implementation of cox model in python with the help of lifelines package. Understanding the impact of features on survival rates helps us in predicting the retention of a customer profile. The Cox model assumes that each feature have an impact on the survival/retention rate.&lt;/p&gt;

&lt;p&gt;One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the Cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls high correlation between covariates.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from lifelines import CoxPHFittr
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df_f, duration_col=’Total number of months in service’, event_col=’churn’)
cph.summary #output2
cph.plot   #output3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;interpreting-the-summary&quot;&gt;Interpreting the summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hazard ratio (HR) given by exp(coef), where coef is the weight corresponding to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &amp;gt; 1, decreases the hazard: &lt;strong&gt;improves the survival&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;number of unique subscribers in the household&lt;/em&gt; has HR = 1.35 which improves survival/retention. &lt;em&gt;mean number of unanswered data calls&lt;/em&gt; has HR = 0.16 implies it has bad effect on survival rate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results--visualization&quot;&gt;Results &amp;amp; Visualization&lt;/h2&gt;

&lt;p&gt;The best way to understand impact of each features/decision is to plot the survival curves for single feature/decision by keeping all other customers characteristics/demographics unchanged. we use [&lt;strong&gt;plot_covariate_groups()](https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.CoxPHFitter.plot_covariate_groups)&lt;/strong&gt; method and pass the arguments — feature of interest and the values to display.&lt;/p&gt;

&lt;h3 id=&quot;survival-profiles-of-features&quot;&gt;Survival Profiles of Features&lt;/h3&gt;

&lt;p&gt;One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival/retention probability than that to its left. A plot below for Average monthly revenue over the life of the customer =400 has more survival probability as it is to the right compared to that of 10 which is to its left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;personalized-strategies-for-customers&quot;&gt;Personalized strategies for Customers&lt;/h3&gt;

&lt;p&gt;We can plot the survival profiles for each customer and analyse the reasons for low survival/retention rates by looking at customer features. From above discussions, we already know what actions can improve the survival rates of the customer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can plot the survival profiles of each customer. For time being let’s consider customer with ID 1032424 and compare the two strategies as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/TP1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see that strategy 1( i&lt;em&gt;ssue more models to the customer : 1032424&lt;/em&gt;) has comparatively longer survival time than strategy 2(Reduce revenue generated from the customer by giving offer). Similarly, we can analyse each and every customer and design proactive strategies to ensure highest retention statistically. We can also compare different strategies developed by business intelligence teams and deploy based on the effectiveness of a strategy to retain customers.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We segmented customer behavior by grouping them based on average monthly revenue brackets, number of models issued etc., and Cox proportional-hazards model enabled us to derive personalized strategies to reduce the churn rate. Not only deriving personalized strategies, we learnt to compare them. As an example, for one customer we saw in above section, statistically proves that issuing more models will have a better impact than providing incentives/offers for a customer to stick to the company for a longer time span. This is an outstanding way to meet the landscape of customer expectations and increase customer engagement with the company.&lt;/p&gt;

&lt;p&gt;Thanks for your time :)&lt;/p&gt;

&lt;p&gt;Full code for reference:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv('Data_Churn_Telecom_Cox.csv')
with open('cols.txt') as f:
   lines = f.readlines()
cols_names = {}
for i in range(len(lines)):
   for j in df.columns:
       if j == lines[i][:-1]:
           cols_names[j] = lines[i+1][:-1]
cols_names['Customer_ID'] = 'Customer_ID'
df.columns =   cols_names.values()


df['churn'] = df['Instance of churn between 31-60 days after observation date']
del  df['Instance of churn between 31-60 days after observation date']
df = df.dropna()

del df['N']
df.set_index('Customer_ID', inplace=True)
'''
Drop categorical features with unique values &amp;gt;2
'''
import numpy as np
df_str = df.loc[:, df.dtypes == object]
for i in df_str.columns:
   if len(np.unique(df_str[i].values)) &amp;gt;2:
       del df[i]
'''
One hot encoding
'''
df_str = df.loc[:, df.dtypes == object]
for i in df_str.columns:
   one_hot = pd.get_dummies(df[i])
   one_hot.columns = [i +'_'+j for j in one_hot.columns]
   df = df.drop(i,axis = 1)
   df = df.join(one_hot)

survival_time = df['Total number of months in service'].values
del df['Total number of months in service']
churn = df['churn'].values
del df['churn']


'''
Drop correlated features
'''

corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.98)]
df.drop(to_drop, axis=1, inplace=True)



df = df[list(df.columns[:69])+['Credit card indicator_N']]
df['Total number of months in service'] = survival_time
df['churn'] = churn
df = df[df['churn'] == 1]

'''
Select valuable features
'''
df_sampled = df.sample(n=1000)
from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.01)
cph.fit(df_sampled, duration_col='Total number of months in service', event_col='churn')
df_stats = cph.summary
features_valuable = list(df_stats[df_stats['exp(coef)'].values &amp;gt; 1.01].index) + list(df_stats[df_stats['exp(coef)'].values &amp;lt; 0.98].index)
df = df[features_valuable+['churn','Total number of months in service']]



from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.01)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>dharani</name></author><category term="CoxProcess" /><summary type="html">Techniques to devise personalized strategies using statistical models</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/TP1-1.jpg" /></entry><entry><title type="html">Game Theory- The Genius of Nash</title><link href="http://localhost:4000/techtara.github.io/Game-Theory-The-Genius-of-Nash/" rel="alternate" type="text/html" title="Game Theory- The Genius of Nash" /><published>2020-06-10T00:00:00+05:30</published><updated>2020-06-10T00:00:00+05:30</updated><id>http://localhost:4000/techtara.github.io/Game%20Theory:%20The%20Genius%20of%20Nash</id><content type="html" xml:base="http://localhost:4000/techtara.github.io/Game-Theory-The-Genius-of-Nash/">&lt;p&gt;We discussed strict dominance solution concept in great detail in the last blog. Its application is limited and only applicable to some section of games( Games with strict dominant strategy). Strict dominant strategy often fails to exist. 
Lets consider &lt;strong&gt;Battle of sexes&lt;/strong&gt; game.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dominant strategy equilibrium did not apply, because there is no dominant strategy. 
In the last blog, we discussed the concept of belief. Player will behave optimally( best response ) to their beliefs. Chris may behave optimally and go to football given his belief that Alex is going to the football game. But their beliefs can be wrong.&lt;/p&gt;

&lt;p&gt;In this blog, we will discuss one of the most central and best known solution concept in the game theory. This overcomes many shortcoming faced by other solution concepts, this is developed by &lt;strong&gt;John Nash&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let’s define Nash’s solution concept. Nash equilibrium is as a profile of strategies(defined in the last blog) for which each player is choosing a best response to the strategies of all other players.
Each strategy in a Nash equilibrium is a best response to all other strategies in that equilibrium
 Lets formally define nash equilibrium&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The pure-strategy profile &lt;strong&gt;s&lt;em&gt;= (s&lt;/em&gt;&lt;/strong&gt;₁&lt;strong&gt;, s&lt;/strong&gt;*₂&lt;strong&gt;, . . . , s&lt;em&gt;n) ∈ S** is a Nash equilibrium if **s&lt;/em&gt;ᵢ&lt;/strong&gt; is a best response to &lt;strong&gt;s*₋ᵢ&lt;/strong&gt; , for all i ∈ N, that is,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;v&lt;/strong&gt;ᵢ&lt;strong&gt;(s∗ᵢ , s∗₋ᵢ) ≥ v&lt;/strong&gt;ᵢ&lt;strong&gt;(sᵢ, s∗₋ᵢ)&lt;/strong&gt; &lt;strong&gt;for all sᵢ ∈ Sᵢ and all i ∈ N.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please note that s* is strategy profile, not strategy. strategy profile refers of set of actions taken by all the players in a strategic environment/game.
lets try to understand this definition by working out an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider this matrix representation. Now lets write down all possible strategy profiles. 
&lt;strong&gt;S&lt;/strong&gt; = {(L,U), (C,U),(R,U),(L,M), (C,M),(R,M),(L,D), (C,D),(R,D)}.
Now lets evaluate payoff functions vis-a-vis best response. 
if player 1 chooses U best response for player 2 is L: BR₂(U) = L
&lt;strong&gt;BR&lt;/strong&gt;₂&lt;strong&gt;(U) = L&lt;/strong&gt;, BR₂(M) = C, BR₂(D) = R
&lt;strong&gt;BR&lt;/strong&gt;₁&lt;strong&gt;(L) = U&lt;/strong&gt;, BR₁(C) = D, BR₁(R) = U
Now closely observe If player 2 chooses L, then player 1’s best response is {U}; at the same time, if player 1 chooses U, then player 2’s best response {L}. It clearly fits the definition above. 
So this is the &lt;strong&gt;s*: {L, U}&lt;/strong&gt; Nash equilibrium. 
let’s apply Nash’s solution concept to prisoners dilemma.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S = {(RS,BE), (BE,BE), (BE,RS), (RS, RS)} 
Nash equilibrium s* is (BE,BE)
I encourage readers to solve this and find out how (BE,BE) is Nash Equilibrium.
Here are the assumptions for a Nash equilibrium:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Each player is playing a best response to his beliefs.&lt;/li&gt;
  &lt;li&gt;The beliefs of the players about their opponents are correct.
We will not dig too deep into these assumptions as it can put us in mid of some philosophical discussion. 
Lets compare Nash solution concept with other solution concepts&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here it easy to deduce that there is strictly dominant strategy for both players: thus strict dominance concept fails.
There is no strictly dominated strategy for any player, so iterative elimination method is not applicable.&lt;/p&gt;

&lt;p&gt;Lets check if a pure-strategy Nash equilibrium does exist.
BR₁(L) = D, &lt;strong&gt;BR&lt;/strong&gt;₁&lt;strong&gt;(C) = M,&lt;/strong&gt; BR₁(R) = M
BR₂(U) = L, &lt;strong&gt;BR&lt;/strong&gt;₂&lt;strong&gt;(M) = C&lt;/strong&gt;, BR₂(D) = L
we find that &lt;strong&gt;(M, C)&lt;/strong&gt; is the &lt;strong&gt;pure-strategy Nash equilibrium&lt;/strong&gt;— and it is unique.&lt;/p&gt;

&lt;p&gt;Solution concept is finest if it predicts or prescribes an unique strategy. It is necessary to understand if Nash equilibrium always yields unique strategy.
Lets consider the battle of sexes game.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s solve Nash equilibrium for this game. 
S = {(O, F), (O, O), (F, F), (F,O )}
BRa(O) = O, BRa(F) = F
BRc(O) = O, BRa(F) = F&lt;/p&gt;

&lt;p&gt;We can clearly observe that we may not have a unique Nash equilibrium, but it usually lead to more refined predictions than those of strict dominant solution concept and iterative elimination. 
Nash equilibrium solution concept has been applied widely in economics, political science, legal studies, and even biology.
Let’s discuss an example where we can apply Nash’s solution concept to real life problem.&lt;/p&gt;

&lt;h2 id=&quot;stag-hunt&quot;&gt;Stag Hunt&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Two individuals go out on a hunt. Each can individually choose to hunt a stag or hunt a hare. Each player must choose an action without knowing the choice of the other. If an individual hunts a stag, they must have the cooperation of their partner in order to succeed. An individual can get a hare by himself, but a hare is worth less than a stag. This has been taken to be a useful analogy for social cooperation, such as international agreements on climate change.The payoff matrix is as follows&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BR₁(S) = S, BR₁(H) = H
BR₂(H) = H, BR₂(S) = S
Game has two pure-strategy equilibria: (S, S) and (H, H). However, the payoff from (S, S) &lt;strong&gt;Pareto dominates&lt;/strong&gt; that from (H, H).&lt;/p&gt;

&lt;p&gt;If a player anticipates that the other individual is not cooperative, then he would choose to hunt a hare. But if he believes that other individual will cooperate then we would choose stag. When both individuals choose stag i.e when both believe other individual will cooperate, as a whole both of them would be better off.&lt;/p&gt;

&lt;h2 id=&quot;scarce-resource&quot;&gt;Scarce Resource&lt;/h2&gt;

&lt;p&gt;Let’s try to understand how self interested players might behave in scenario of scarce resources. Imaging there are &lt;strong&gt;n&lt;/strong&gt; fertiliser manufacturing companies each choosing how much to produce around a fresh water lake. Each manufacturing companies degrades some amount of fresh water in that lake and uses, Lets say the total units of water in lake is K. Each player i chooses his own consumption of clean water for production, k&lt;strong&gt;ᵢ&lt;/strong&gt; ≥ 0, and the amount of clean water left is therefore &lt;strong&gt;K -⅀ki&lt;/strong&gt; .
The benefit of consuming an amount k&lt;strong&gt;ᵢ&lt;/strong&gt; ≥ 0 gives player i a benefit equal to &lt;strong&gt;ln(kᵢ)&lt;/strong&gt; to the fertiliser company, and no other player benefits from i’s choice.
Each player also enjoys consuming the remainder of the clean air, giving each a benefit ln(K −&lt;strong&gt;⅀&lt;/strong&gt; kj). Hence the total payoff of player i is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For player i from the choice k= (k₁, k₂, . . . , kn).
To compute Nash equilibrium, we need to find a strategy profile for which all players choose best-response to their beliefs about his opponent). 
That is we find strategy profile (k∗₁, k∗₂, . . . , k∗n) for which k∗&lt;strong&gt;ᵢ&lt;/strong&gt;= BRi(k∗&lt;strong&gt;₋ᵢ&lt;/strong&gt;) for all i ∈ N. For player I, we can get best response the by maximising the value function written above. To find ki, which maximises the value function of industry i, We can equate its derivative to zero.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Solving above equation gives player’s i best response.&lt;/p&gt;

&lt;p&gt;Lets take only 2 industries case and solve this. ki(kj ) be the best response of player i.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets plot this with k1 payoff in x axis and k2 payoff in y axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we solve the two best-response functions simultaneously, we find the unique Nash equilibrium, which has both players playing k₁= k₂ = K/3.&lt;/p&gt;

&lt;h2 id=&quot;mixed-strategies&quot;&gt;&lt;strong&gt;Mixed Strategies&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;So far we discussed pure strategies, but we need to discuss the problem where player may choose to randomise between several of his pure strategies. There are many interesting applications to this kind of behaviour where player chooses actions stochastically( i.e. Instead of chooses a single strategy, player chooses a distribution of strategies). The probability of choosing any of pure strategy is nonnegative, and the sum of the probabilities of choosing any all pure strategies events must add up to one.
We will also closely observe applicability of Nash equilibrium to these mixed strategies. In fact, Nash equilibrium is applied to the games only if player chooses mixed strategies instead of pure strategies.&lt;/p&gt;

&lt;p&gt;We start with the basic definition of random play when players have finite strategy sets &lt;strong&gt;Sᵢ&lt;/strong&gt;:
Let &lt;strong&gt;Sᵢ = {sᵢ₁, sᵢ₁, . . . , sᵢm}&lt;/strong&gt; be player i’s finite set of pure strategies. Define &lt;strong&gt;ΔSᵢ **as the simplex of **Sᵢ&lt;/strong&gt;, which is the set of all probability distributions over &lt;strong&gt;Sᵢ&lt;/strong&gt; . A mixed strategy for player i is an element &lt;strong&gt;σᵢ ∈ Sᵢ&lt;/strong&gt;, so that
&lt;strong&gt;σᵢ= {σ(sᵢ₁), σᵢ(sᵢ₂), . . . , σᵢ(sᵢm))&lt;/strong&gt; is a probability distribution over &lt;strong&gt;Sᵢ **,
where **σᵢ(sᵢ)&lt;/strong&gt; is the probability that player i plays s&lt;strong&gt;ᵢ&lt;/strong&gt; .&lt;/p&gt;

&lt;p&gt;Now consider the example of the rock-paper-scissors game, in which S&lt;strong&gt;ᵢ&lt;/strong&gt;= {R, P, S} (for rock, paper, and scissors, respectively). We can define the simplex as 
ΔSi ={(σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P ), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)) : σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P ), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)≥0, σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R)+σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P )+σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)=1},&lt;/p&gt;

&lt;p&gt;The player i and his opponents -i both choose mixed actions. It implies that player’s i belief about his opponents -i is not fixed but random. Thus a belief for player i is a probability distribution over the strategies of his opponents.&lt;/p&gt;

&lt;p&gt;Definition: A &lt;strong&gt;belief&lt;/strong&gt; for player i is given by a probability distribution &lt;strong&gt;πᵢ∈S₋ᵢ&lt;/strong&gt; over the strategies of his opponents. We denote by &lt;strong&gt;πᵢ(s₋ᵢ)&lt;/strong&gt; the probability player i assigns to his opponents playing &lt;strong&gt;s₋ᵢ ∈ S₋ᵢ&lt;/strong&gt; .
For example in the rock-paper-scissors game, Belief of player i is represented as (&lt;strong&gt;πᵢ&lt;/strong&gt;(R), &lt;strong&gt;πᵢ&lt;/strong&gt;(P ), &lt;strong&gt;πᵢ&lt;/strong&gt;(S)). We can think of σ&lt;strong&gt;*₋ᵢ&lt;/strong&gt; as the belief of player i about his opponents, π&lt;strong&gt;ᵢ&lt;/strong&gt;, which captures the idea that player i is uncertain of his opponents.&lt;/p&gt;

&lt;p&gt;behavior.&lt;/p&gt;

&lt;p&gt;In pure strategy, the payoff is straight forward. In mixed strategy, to evaluate payoff we need to reintroduce the concept of &lt;strong&gt;expected payoff.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The expected payoff of player i when he chooses pure strategy &lt;strong&gt;sᵢ∈ Sᵢ&lt;/strong&gt; and his opponents choose mixed strategy &lt;strong&gt;σ₋ᵢ∈ ΔS−ᵢ&lt;/strong&gt;
Please note that pure strategy is part of mixed strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-14.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When player i choose mixed strategy &lt;strong&gt;σᵢ∈ ΔS&lt;/strong&gt;ᵢ and his opponents choose mixed strategy &lt;strong&gt;σ₋ᵢ∈ ΔS₋ᵢ.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/techtara.github.io/assets/images/B2-15.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let calculate payoff in mixed strategy scenario.&lt;/p&gt;

&lt;p&gt;lets assume that player 2 plays σ₂(R) = 0.5
σ₂(P ) = 0.5
σ₂(S) = 0 
We can now calculate the expected payoff for player 1 if he chooses pure strategy.
V₁(R, σ₂) = 0.5&lt;em&gt;(0)+ 0.5&lt;/em&gt;(-1) + 0 &lt;em&gt;(1)=-0.5
V₁(P, σ₂) = 0.5&lt;/em&gt;(1)+ 0.5&lt;em&gt;(0) + 0 *(-1)= 0.5
V₁(S, σ₂) = 0.5&lt;/em&gt;(-1)+ 0.5*(1) + 0 *(0)=0&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;V₁(P, σ2)&amp;gt;V₁(S, σ2)&amp;gt;V₁(R, σ₂)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To given player 2’s mixed strategy, we see a best response to player 1, which is action P.&lt;/p&gt;

&lt;p&gt;Now let’s understand how Nash equilibrium solution concept applies to mixed strategies. It actually simpler than it looks, we just replace strategy profile with mixed strategy profile.
&lt;strong&gt;Definition:&lt;/strong&gt; The mixed-strategy profile &lt;strong&gt;σ* = (σ&lt;em&gt;₁ , σ&lt;/em&gt;₂ , . . . , σ&lt;em&gt;n )** is a Nash equilibrium if for each player **σ&lt;/em&gt;ᵢ **is a best response to **σ&lt;em&gt;₋ᵢ**. That is, for all i ∈ N, **vᵢ(σ&lt;/em&gt;ᵢ , σ&lt;em&gt;₋ᵢ) ≥ vᵢ(σᵢ, σ&lt;/em&gt;₋ᵢ). ∀ σᵢ∈ Sᵢ&lt;/strong&gt;.
Each mixed strategy in a Nash equilibrium is a best response to all other mixed strategies in that equilibrium.&lt;/p&gt;

&lt;p&gt;Let’s close the discussion on mixed strategies here. We will discuss more about them in the next blog in my blog series. 
Hope you enjoy reading this blog.
Thanks :)&lt;/p&gt;</content><author><name>kowshik</name></author><category term="Game" /><category term="Theory" /><summary type="html">We discussed strict dominance solution concept in great detail in the last blog. Its application is limited and only applicable to some section of games( Games with strict dominant strategy). Strict dominant strategy often fails to exist. Lets consider Battle of sexes game.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/techtara.github.io/assets/images/B2-1.jpg" /></entry></feed>